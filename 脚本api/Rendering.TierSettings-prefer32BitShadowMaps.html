<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <link href="./css/stylesheet.css" rel="stylesheet" type="text/css"><title>Unity - Scripting API: Rendering.TierSettings.prefer32BitShadowMaps</title></head> <body> <div class="content"><div class="section"><div class="mb20 clear" id=""><h1 class="heading inherit"><a href="Rendering.TierSettings.html">TierSettings</a>.prefer32BitShadowMaps</h1><div class="clear"></div><div class="clear"></div><div class="suggest"><div class="suggest-wrap rel hide"><div class="loading hide"><div></div><div></div><div></div></div></div></div><a href="" class="switch-link gray-btn sbtn left hide"></a><div class="clear"></div></div><div class="subsection"><div class="signature"><div class="signature-CS sig-block"><span style="color:red;"></span>public bool <span class="sig-kw">prefer32BitShadowMaps</span>;
    </div></div></div><div class="subsection"><h2>Description   描述</h2><p>Allows you to specify whether Unity should try to use 32-bit shadow maps, where possible.</p></div><div class="subsection"><p>Most platforms have a fixed shadow map format that you cannot adjust. These vary in format, and can be 16-bit, 24-bit, or 32-bit, and can also be either float or integer based.<br /><br />However, when you are targeting PlayStation 4 or platforms using DirectX 11 or DirectX 12, you can choose whether Unity should use a 16-bit or 32-bit float shadow map.<br /><br />32-bit shadow maps give higher quality shadows than 16-bit, but they will use increased memory and bandwidth on the GPU.<br /><br />In addition, 32-bit shadow maps can only be used if the <a href="Display-depthBuffer.html">depth buffer</a> is also set to 32-bit.</p></div></div></div></div></body></html>